{
    "name": "Troubleshooting CRI-O container runtime issues (from Red Hat OCP 4.9) by tonino test1",
    "description": "Source: https://docs.openshift.com/container-platform/4.9/support/troubleshooting/troubleshooting-crio-issues.html",
    "steps": [
        {
            "number": 1,
            "title": "DISCLAIMER",
            "description": "This runbook has been generated from <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://docs.openshift.com/container-platform/4.9/support/troubleshooting/troubleshooting-crio-issues.html\">https://docs.openshift.com/container-platform/4.9/support/troubleshooting/troubleshooting-crio-issues.html</a> and should be reviewed before it is used in production. Once you have validated the content and the format of this runbook, you can remove this step and publish the runbook.<br>The source document contains 1 images that have not been copied into this runbook. You can add the images manually. Large image files should be reduced in file size (e.g., by taking a screen shot), as runbooks must have a total size of at most 1 MB. This is the list of images in the source document: <ul><li>https://assets.openshift.com/hubfs/images/logos/Logo-RedHat-A-Reverse-RGB.svg</li></ul>",
            "type": "manual"
        },
        {
            "title": "About CRI-O container runtime engine",
            "description": "<p>CRI-O is a Kubernetes-native container runtime implementation that integrates closely with the operating system to deliver an efficient and optimized Kubernetes experience. CRI-O provides facilities for running, stopping, and restarting containers.</p><p>The CRI-O container runtime engine is managed using a systemd service on each OpenShift Container Platform cluster node. When container runtime issues occur, verify the status of the crio systemd service on each node. Gather CRI-O journald unit logs from nodes that manifest container runtime issues.</p>",
            "number": 2,
            "type": "manual"
        },
        {
            "title": "Verifying CRI-O runtime engine status",
            "description": "<p>You can verify CRI-O container runtime engine status on each cluster node.</p><ul> <li> <p>You have access to the cluster as a user with the <code>cluster-admin</code> role.</p> </li> <li> <p>You have installed the OpenShift CLI (<code>oc</code>).</p> </li> </ul><ol> <li> <p>Review CRI-O status by querying the <code>crio</code> systemd service on a node, within a debug pod.</p> <div> <ol type=\"a\"> <li> <p>Start a debug pod for a node:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc debug node/my-node</code></pre> </div> </div> </li> <li> <p>Set <code>/host</code> as the root directory within the debug shell. The debug pod mounts the host’s root file system in <code>/host</code> within the pod. By changing the root directory to <code>/host</code>, you can run binaries contained in the host’s executable paths:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>#</span><span> </span><span>chroot</span> /host</code></pre> </div> </div> <div> <table> <tbody><tr> <td> <i title=\"Note\"></i> </td> <td> <div> <p>OpenShift Container Platform 4.9 cluster nodes running Red Hat Enterprise Linux CoreOS (RHCOS) are immutable and rely on Operators to apply cluster changes. Accessing cluster nodes using SSH is not recommended and nodes will be tainted as <em>accessed</em>. However, if the OpenShift Container Platform API is not available, or the kubelet is not properly functioning on the target node, <code>oc</code> operations will be impacted. In such situations, it is possible to access nodes using <code>ssh core@&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code> instead.</p> </div> </td> </tr> </tbody></table> </div> </li> <li> <p>Check whether the <code>crio</code> systemd service is active on the node:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>#</span><span> </span>systemctl is-active crio</code></pre> </div> </div> </li> <li> <p>Output a more detailed <code>crio.service</code> status summary:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>#</span><span> </span>systemctl status crio.service</code></pre> </div> </div> </li> </ol> </div> </li> </ol>",
            "number": 3,
            "type": "manual"
        },
        {
            "title": "Gathering CRI-O journald unit logs",
            "description": "<p>If you experience CRI-O issues, you can obtain CRI-O journald unit logs from a node.</p><ul> <li> <p>You have access to the cluster as a user with the <code>cluster-admin</code> role.</p> </li> <li> <p>Your API service is still functional.</p> </li> <li> <p>You have installed the OpenShift CLI (<code>oc</code>).</p> </li> <li> <p>You have the fully qualified domain names of the control plane or control plane machines.</p> </li> </ul><ol> <li> <p>Gather CRI-O journald unit logs. The following example collects logs from all control plane nodes (within the cluster:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc adm node-logs <span>--role</span><span>=</span>master <span>-u</span> crio</code></pre> </div> </div> </li> <li> <p>Gather CRI-O journald unit logs from a specific node:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc adm node-logs &lt;node_name&gt; <span>-u</span> crio</code></pre> </div> </div> </li> <li> <p>If the API is not functional, review the logs using SSH instead. Replace <code>&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code> with appropriate values:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh core@&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; journalctl <span>-b</span> <span>-f</span> <span>-u</span> crio.service</code></pre> </div> </div> <div> <table> <tbody><tr> <td> <i title=\"Note\"></i> </td> <td> <div> <p>OpenShift Container Platform 4.9 cluster nodes running Red Hat Enterprise Linux CoreOS (RHCOS) are immutable and rely on Operators to apply cluster changes. Accessing cluster nodes using SSH is not recommended and nodes will be tainted as <em>accessed</em>. Before attempting to collect diagnostic data over SSH, review whether the data collected by running <code>oc adm must gather</code> and other <code>oc</code> commands is sufficient instead. However, if the OpenShift Container Platform API is not available, or the kubelet is not properly functioning on the target node, <code>oc</code> operations will be impacted. In such situations, it is possible to access nodes using <code>ssh core@&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code>.</p> </div> </td> </tr> </tbody></table> </div> </li> </ol>",
            "number": 4,
            "type": "manual"
        },
        {
            "title": "Cleaning CRI-O storage",
            "description": "<p>You can manually clear the CRI-O ephemeral storage if you experience the following issues:</p><ul> <li> <p>A node cannot run on any pods and this error appears:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>Failed to create pod sandbox: rpc error: code = Unknown desc = failed to mount container XXX: error recreating the missing symlinks: error reading name of symlink for XXX: open /var/lib/containers/storage/overlay/XXX/link: no such file or directory</span></code></pre> </div> </div> </li> <li> <p>You cannot create a new container on a working node and the “can’t stat lower layer” error appears:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>can't stat lower layer ...  because it does not exist.  Going through storage to recreate the missing symlinks.</span></code></pre> </div> </div> </li> <li> <p>Your node is in the <code>NotReady</code> state after a cluster upgrade or if you attempt to reboot it.</p> </li> <li> <p>The container runtime implementation (<code>crio</code>) is not working properly.</p> </li> <li> <p>You are unable to start a debug shell on the node using <code>oc debug node/&lt;nodename&gt;</code> because the container runtime instance (<code>crio</code>) is not working.</p> </li> </ul><p>Follow this process to completely wipe the CRI-O storage and resolve the errors.</p><ul> <li> <p>You have access to the cluster as a user with the <code>cluster-admin</code> role.</p> </li> <li> <p>You have installed the OpenShift CLI (<code>oc</code>).</p> </li> </ul><ol> <li> <p>Use <code>cordon</code> on the node. This is to avoid any workload getting scheduled if the node gets into the <code>Ready</code> status. You will know that scheduling is disabled when <code>SchedulingDisabled</code> is in your Status section:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc adm cordon &lt;nodename&gt;</code></pre> </div> </div> </li> <li> <p>Drain the node as the cluster-admin user:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc adm drain &lt;nodename&gt; <span>--ignore-daemonsets</span> <span>--delete-local-data</span></code></pre> </div> </div> </li> <li> <p>When the node returns, connect back to the node via SSH or Console. Then connect to the root user:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh core@node1.example.com\n<span>$</span><span> </span><span>sudo</span> <span>-i</span></code></pre> </div> </div> </li> <li> <p>Manually stop the kubelet:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>#</span><span> </span>systemctl stop kubelet</code></pre> </div> </div> </li> <li> <p>Stop the containers and pods:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>#</span><span> </span>crictl rmp <span>-fa</span></code></pre> </div> </div> </li> <li> <p>Manually stop the crio services:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>#</span><span> </span>systemctl stop crio</code></pre> </div> </div> </li> <li> <p>After you run those commands, you can completely wipe the ephemeral storage:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>#</span><span> </span>crio wipe <span>-f</span></code></pre> </div> </div> </li> <li> <p>Start the crio and kubelet service:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>#</span><span> </span>systemctl start crio\n<span>#</span><span> </span>systemctl start kubelet</code></pre> </div> </div> </li> <li> <p>You will know if the clean up worked if the crio and kubelet services are started, and the node is in the <code>Ready</code> status:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc get nodes</code></pre> </div> </div> <div> <div>Example output</div> <div> <pre><code data-lang=\"terminal\"><span>NAME\t\t\t\t    STATUS\t                ROLES    AGE    VERSION\nci-ln-tkbxyft-f76d1-nvwhr-master-1  Ready, SchedulingDisabled   master\t 133m   v1.22.0-rc.0+75ee307</span></code></pre> </div> </div> </li> <li> <p>Mark the node schedulable. You will know that the scheduling is enabled when <code>SchedulingDisabled</code> is no longer in status:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc adm uncordon &lt;nodename&gt;</code></pre> </div> </div> <div> <div>Example output</div> <div> <pre><code data-lang=\"terminal\"><span>NAME\t\t\t\t     STATUS\t      ROLES    AGE    VERSION\nci-ln-tkbxyft-f76d1-nvwhr-master-1   Ready            master   133m   v1.22.0-rc.0+75ee307</span></code></pre> </div> </div> </li> </ol>",
            "number": 5,
            "type": "manual"
        },
        {
            "title": "Cleaning CRI-O storage",
            "description": "<p>You can manually clear the CRI-O ephemeral storage if you experience the following issues:</p><ul> <li> <p>A node cannot run on any pods and this error appears:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>Failed to create pod sandbox: rpc error: code = Unknown desc = failed to mount container XXX: error recreating the missing symlinks: error reading name of symlink for XXX: open /var/lib/containers/storage/overlay/XXX/link: no such file or directory</span></code></pre> </div> </div> </li> <li> <p>You cannot create a new container on a working node and the “can’t stat lower layer” error appears:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>can't stat lower layer ...  because it does not exist.  Going through storage to recreate the missing symlinks.</span></code></pre> </div> </div> </li> <li> <p>Your node is in the <code>NotReady</code> state after a cluster upgrade or if you attempt to reboot it.</p> </li> <li> <p>The container runtime implementation (<code>crio</code>) is not working properly.</p> </li> <li> <p>You are unable to start a debug shell on the node using <code>oc debug node/&lt;nodename&gt;</code> because the container runtime instance (<code>crio</code>) is not working.</p> </li> </ul><p>Follow this process to completely wipe the CRI-O storage and resolve the errors.</p><ul> <li> <p>You have access to the cluster as a user with the <code>cluster-admin</code> role.</p> </li> <li> <p>You have installed the OpenShift CLI (<code>oc</code>).</p> </li> </ul><ol> <li> <p>Use <code>cordon</code> on the node. This is to avoid any workload getting scheduled if the node gets into the <code>Ready</code> status. You will know that scheduling is disabled when <code>SchedulingDisabled</code> is in your Status section:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc adm cordon &lt;nodename&gt;</code></pre> </div> </div> </li> <li> <p>Drain the node as the cluster-admin user:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc adm drain &lt;nodename&gt; <span>--ignore-daemonsets</span> <span>--delete-local-data</span></code></pre> </div> </div> </li> <li> <p>When the node returns, connect back to the node via SSH or Console. Then connect to the root user:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh core@node1.example.com\n<span>$</span><span> </span><span>sudo</span> <span>-i</span></code></pre> </div> </div> </li> <li> <p>Manually stop the kubelet:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>#</span><span> </span>systemctl stop kubelet</code></pre> </div> </div> </li> <li> <p>Stop the containers and pods:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>#</span><span> </span>crictl rmp <span>-fa</span></code></pre> </div> </div> </li> <li> <p>Manually stop the crio services:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>#</span><span> </span>systemctl stop crio</code></pre> </div> </div> </li> <li> <p>After you run those commands, you can completely wipe the ephemeral storage:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>#</span><span> </span>crio wipe <span>-f</span></code></pre> </div> </div> </li> <li> <p>Start the crio and kubelet service:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>#</span><span> </span>systemctl start crio\n<span>#</span><span> </span>systemctl start kubelet</code></pre> </div> </div> </li> <li> <p>You will know if the clean up worked if the crio and kubelet services are started, and the node is in the <code>Ready</code> status:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc get nodes</code></pre> </div> </div> <div> <div>Example output</div> <div> <pre><code data-lang=\"terminal\"><span>NAME\t\t\t\t    STATUS\t                ROLES    AGE    VERSION\nci-ln-tkbxyft-f76d1-nvwhr-master-1  Ready, SchedulingDisabled   master\t 133m   v1.22.0-rc.0+75ee307</span></code></pre> </div> </div> </li> <li> <p>Mark the node schedulable. You will know that the scheduling is enabled when <code>SchedulingDisabled</code> is no longer in status:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc adm uncordon &lt;nodename&gt;</code></pre> </div> </div> <div> <div>Example output</div> <div> <pre><code data-lang=\"terminal\"><span>NAME\t\t\t\t     STATUS\t      ROLES    AGE    VERSION\nci-ln-tkbxyft-f76d1-nvwhr-master-1   Ready            master   133m   v1.22.0-rc.0+75ee307</span></code></pre> </div> </div> </li> </ol>",
            "number": 6,
            "type": "manual"
        }
    ],
    "tags": [
        "converted-ocp-troubleshooting"
    ]
}
