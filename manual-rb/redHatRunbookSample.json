{
    "name": "Troubleshooting installations (from Red Hat OCP 4.9)",
    "description": "Source: https://docs.openshift.com/container-platform/4.9/support/troubleshooting/troubleshooting-installations.html",
    "steps": [
        {
            "number": 1,
            "title": "DISCLAIMER",
            "description": "This runbook has been generated from <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://docs.openshift.com/container-platform/4.9/support/troubleshooting/troubleshooting-installations.html\">https://docs.openshift.com/container-platform/4.9/support/troubleshooting/troubleshooting-installations.html</a> and should be reviewed before it is used in production. Once you have validated the content and the format of this runbook, you can remove this step and publish the runbook.<br>The source document contains 1 images that have not been copied into this runbook. You can add the images manually. Large image files should be reduced in file size (e.g., by taking a screen shot), as runbooks must have a total size of at most 1 MB. This is the list of images in the source document: <ul><li>https://assets.openshift.com/hubfs/images/logos/Logo-RedHat-A-Reverse-RGB.svg</li></ul>",
            "type": "manual"
        },
        {
            "title": "Determining where installation issues occur",
            "description": "<p>When troubleshooting OpenShift Container Platform installation issues, you can monitor installation logs to determine at which stage issues occur. Then, retrieve diagnostic data relevant to that stage.</p><p>OpenShift Container Platform installation proceeds through the following stages:</p><ol> <li> <p>Ignition configuration files are created.</p> </li> <li> <p>The bootstrap machine boots and starts hosting the remote resources required for the control plane machines to boot.</p> </li> <li> <p>The control plane machines fetch the remote resources from the bootstrap machine and finish booting.</p> </li> <li> <p>The control plane machines use the bootstrap machine to form an etcd cluster.</p> </li> <li> <p>The bootstrap machine starts a temporary Kubernetes control plane using the new etcd cluster.</p> </li> <li> <p>The temporary control plane schedules the production control plane to the control plane machines.</p> </li> <li> <p>The temporary control plane shuts down and passes control to the production control plane.</p> </li> <li> <p>The bootstrap machine adds OpenShift Container Platform components into the production control plane.</p> </li> <li> <p>The installation program shuts down the bootstrap machine.</p> </li> <li> <p>The control plane sets up the worker nodes.</p> </li> <li> <p>The control plane installs additional services in the form of a set of Operators.</p> </li> <li> <p>The cluster downloads and configures remaining components needed for the day-to-day operation, including the creation of worker machines in supported environments.</p> </li> </ol>",
            "number": 2,
            "type": "manual"
        },
        {
            "title": "User-provisioned infrastructure installation considerations",
            "description": "<p>The default installation method uses installer-provisioned infrastructure. With installer-provisioned infrastructure clusters, OpenShift Container Platform manages all aspects of the cluster, including the operating system itself. If possible, use this feature to avoid having to provision and maintain the cluster infrastructure.</p><p>You can alternatively install OpenShift Container Platform 4.9 on infrastructure that you provide. If you use this installation method, follow user-provisioned infrastructure installation documentation carefully. Additionally, review the following considerations before the installation:</p><ul> <li> <p>Check the <a href=\"https://access.redhat.com/ecosystem/search/#/ecosystem/Red%20Hat%20Enterprise%20Linux\">Red Hat Enterprise Linux (RHEL) Ecosystem</a> to determine the level of Red Hat Enterprise Linux CoreOS (RHCOS) support provided for your chosen server hardware or virtualization technology.</p> </li> <li> <p>Many virtualization and cloud environments require agents to be installed on guest operating systems. Ensure that these agents are installed as a containerized workload deployed through a daemon set.</p> </li> <li> <p>Install cloud provider integration if you want to enable features such as dynamic storage, on-demand service routing, node hostname to Kubernetes hostname resolution, and cluster autoscaling.</p> <div> <table> <tbody><tr> <td> <i title=\"Note\"></i> </td> <td> <div> <p>It is not possible to enable cloud provider integration in OpenShift Container Platform environments that mix resources from different cloud providers, or that span multiple physical or virtual platforms. The node life cycle controller will not allow nodes that are external to the existing provider to be added to a cluster, and it is not possible to specify more than one cloud provider integration.</p> </div> </td> </tr> </tbody></table> </div> </li> <li> <p>A provider-specific Machine API implementation is required if you want to use machine sets or autoscaling to automatically provision OpenShift Container Platform cluster nodes.</p> </li> <li> <p>Check whether your chosen cloud provider offers a method to inject Ignition configuration files into hosts as part of their initial deployment. If they do not, you will need to host Ignition configuration files by using an HTTP server. The steps taken to troubleshoot Ignition configuration file issues will differ depending on which of these two methods is deployed.</p> </li> <li> <p>Storage needs to be manually provisioned if you want to leverage optional framework components such as the embedded container registry, ElasticSearch, or Prometheus. Default storage classes are not defined in user-provisioned infrastructure installations unless explicitly configured.</p> </li> <li> <p>A load balancer is required to distribute API requests across all control plane nodes in highly available OpenShift Container Platform environments. You can use any TCP-based load balancing solution that meets OpenShift Container Platform DNS routing and port requirements.</p> </li> </ul>",
            "number": 3,
            "type": "manual"
        },
        {
            "title": "Checking a load balancer configuration before OpenShift Container Platform installation",
            "description": "<p>Check your load balancer configuration prior to starting an OpenShift Container Platform installation.</p><ul> <li> <p>You have configured an external load balancer of your choosing, in preparation for an OpenShift Container Platform installation. The following example is based on a Red Hat Enterprise Linux (RHEL) host using HAProxy to provide load balancing services to a cluster.</p> </li> <li> <p>You have configured DNS in preparation for an OpenShift Container Platform installation.</p> </li> <li> <p>You have SSH access to your load balancer.</p> </li> </ul><ol> <li> <p>Check that the <code>haproxy</code> systemd service is active:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh &lt;user_name&gt;@&lt;load_balancer&gt; systemctl status haproxy</code></pre> </div> </div> </li> <li> <p>Verify that the load balancer is listening on the required ports. The following example references ports <code>80</code>, <code>443</code>, <code>6443</code>, and <code>22623</code>.</p> <div> <ul> <li> <p>For HAProxy instances running on Red Hat Enterprise Linux (RHEL) 6, verify port status by using the <code>netstat</code> command:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh &lt;user_name&gt;@&lt;load_balancer&gt; netstat <span>-nltupe</span> | <span>grep</span> <span>-E</span> <span>':80|:443|:6443|:22623'</span></code></pre> </div> </div> </li> <li> <p>For HAProxy instances running on Red Hat Enterprise Linux (RHEL) 7 or 8, verify port status by using the <code>ss</code> command:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh &lt;user_name&gt;@&lt;load_balancer&gt; ss <span>-nltupe</span> | <span>grep</span> <span>-E</span> <span>':80|:443|:6443|:22623'</span></code></pre> </div> </div> <div> <table> <tbody><tr> <td> <i title=\"Note\"></i> </td> <td> <div> <p>Red Hat recommends the <code>ss</code> command instead of <code>netstat</code> in Red Hat Enterprise Linux (RHEL) 7 or later. <code>ss</code> is provided by the iproute package. For more information on the <code>ss</code> command, see the <a href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/performance_tuning_guide/sect-red_hat_enterprise_linux-performance_tuning_guide-tool_reference-ss\">Red Hat Enterprise Linux (RHEL) 7 Performance Tuning Guide</a>.</p> </div> </td> </tr> </tbody></table> </div> </li> </ul> </div> </li> <li> <p>Check that the wildcard DNS record resolves to the load balancer:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>dig &lt;wildcard_fqdn&gt; @&lt;dns_server&gt;</code></pre> </div> </div> </li> </ol>",
            "number": 4,
            "type": "manual"
        },
        {
            "title": "Specifying OpenShift Container Platform installer log levels",
            "description": "<p>By default, the OpenShift Container Platform installer log level is set to info. If more detailed logging is required when diagnosing a failed OpenShift Container Platform installation, you can increase the openshift-install log level to debug when starting the installation again.</p><ul> <li> <p>You have access to the installation host.</p> </li> </ul><ul> <li> <p>Set the installation log level to <code>debug</code> when initiating the installation:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>./openshift-install <span>--dir</span> &lt;installation_directory&gt; wait-for bootstrap-complete <span>--log-level</span> debug  <i data-value=\"1\"></i><b>(1)</b></code></pre> </div> </div> <div> <table> <tbody><tr> <td><i data-value=\"1\"></i><b>1</b></td> <td>Possible log levels include <code>info</code>, <code>warn</code>, <code>error,</code> and <code>debug</code>.</td> </tr> </tbody></table> </div> </li> </ul>",
            "number": 5,
            "type": "manual"
        },
        {
            "title": "Troubleshooting openshift-install command issues",
            "description": "<p>If you experience issues running the openshift-install command, check the following:</p><ul> <li> <p>The installation has been initiated within 24 hours of Ignition configuration file creation. The Ignition files are created when the following command is run:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>./openshift-install create ignition-configs <span>--dir</span><span>=</span>./install_dir</code></pre> </div> </div> </li> <li> <p>The <code>install-config.yaml</code> file is in the same directory as the installer. If an alternative installation path is declared by using the <code>./openshift-install --dir</code> option, verify that the <code>install-config.yaml</code> file exists within that directory.</p> </li> </ul>",
            "number": 6,
            "type": "manual"
        },
        {
            "title": "Monitoring installation progress",
            "description": "<p>You can monitor high-level installation, bootstrap, and control plane logs as an OpenShift Container Platform installation progresses. This provides greater visibility into how an installation progresses and helps identify the stage at which an installation failure occurs.</p><ul> <li> <p>You have access to the cluster as a user with the <code>cluster-admin</code> role.</p> </li> <li> <p>You have installed the OpenShift CLI (<code>oc</code>).</p> </li> <li> <p>You have SSH access to your hosts.</p> </li> <li> <p>You have the fully qualified domain names of the bootstrap and control plane nodes.</p> <div> <table> <tbody><tr> <td> <i title=\"Note\"></i> </td> <td> <div> <p>The initial <code>kubeadmin</code> password can be found in <code>&lt;install_directory&gt;/auth/kubeadmin-password</code> on the installation host.</p> </div> </td> </tr> </tbody></table> </div> </li> </ul><ol> <li> <p>Watch the installation log as the installation progresses:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span><span>tail</span> <span>-f</span> ~/&lt;installation_directory&gt;/.openshift_install.log</code></pre> </div> </div> </li> <li> <p>Monitor the <code>bootkube.service</code> journald unit log on the bootstrap node, after it has booted. This provides visibility into the bootstrapping of the first control plane. Replace <code>&lt;bootstrap_fqdn&gt;</code> with the bootstrap node’s fully qualified domain name:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh core@&lt;bootstrap_fqdn&gt; journalctl <span>-b</span> <span>-f</span> <span>-u</span> bootkube.service</code></pre> </div> </div> <div> <table> <tbody><tr> <td> <i title=\"Note\"></i> </td> <td> <div> <p>The <code>bootkube.service</code> log on the bootstrap node outputs etcd <code>connection refused</code> errors, indicating that the bootstrap server is unable to connect to etcd on control plane nodes. After etcd has started on each control plane node and the nodes have joined the cluster, the errors should stop.</p> </div> </td> </tr> </tbody></table> </div> </li> <li> <p>Monitor <code>kubelet.service</code> journald unit logs on control plane nodes, after they have booted. This provides visibility into control plane node agent activity.</p> <div> <ol type=\"a\"> <li> <p>Monitor the logs using <code>oc</code>:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc adm node-logs <span>--role</span><span>=</span>master <span>-u</span> kubelet</code></pre> </div> </div> </li> <li> <p>If the API is not functional, review the logs using SSH instead. Replace <code>&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code> with appropriate values:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; journalctl <span>-b</span> <span>-f</span> <span>-u</span> kubelet.service</code></pre> </div> </div> </li> </ol> </div> </li> <li> <p>Monitor <code>crio.service</code> journald unit logs on control plane nodes, after they have booted. This provides visibility into control plane node CRI-O container runtime activity.</p> <div> <ol type=\"a\"> <li> <p>Monitor the logs using <code>oc</code>:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc adm node-logs <span>--role</span><span>=</span>master <span>-u</span> crio</code></pre> </div> </div> </li> <li> <p>If the API is not functional, review the logs using SSH instead. Replace <code>&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code> with appropriate values:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh core@master-N.cluster_name.sub_domain.domain journalctl <span>-b</span> <span>-f</span> <span>-u</span> crio.service</code></pre> </div> </div> </li> </ol> </div> </li> </ol>",
            "number": 7,
            "type": "manual"
        },
        {
            "title": "Gathering bootstrap node diagnostic data",
            "description": "<p>When experiencing bootstrap-related issues, you can gather bootkube.service journald unit logs and container logs from the bootstrap node.</p><ul> <li> <p>You have SSH access to your bootstrap node.</p> </li> <li> <p>You have the fully qualified domain name of the bootstrap node.</p> </li> <li> <p>If you are hosting Ignition configuration files by using an HTTP server, you must have the HTTP server’s fully qualified domain name and the port number. You must also have SSH access to the HTTP host.</p> </li> </ul><ol> <li> <p>If you have access to the bootstrap node’s console, monitor the console until the node reaches the login prompt.</p> </li> <li> <p>Verify the Ignition file configuration.</p> <div> <ul> <li> <p>If you are hosting Ignition configuration files by using an HTTP server.</p> <div> <ol type=\"a\"> <li> <p>Verify the bootstrap node Ignition file URL. Replace <code>&lt;http_server_fqdn&gt;</code> with HTTP server’s fully qualified domain name:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>curl <span>-I</span> http://&lt;http_server_fqdn&gt;:&lt;port&gt;/bootstrap.ign  <i data-value=\"1\"></i><b>(1)</b></code></pre> </div> </div> <div> <table> <tbody><tr> <td><i data-value=\"1\"></i><b>1</b></td> <td>The <code>-I</code> option returns the header only. If the Ignition file is available on the specified URL, the command returns <code>200 OK</code> status. If it is not available, the command returns <code>404 file not found</code>.</td> </tr> </tbody></table> </div> </li> <li> <p>To verify that the Ignition file was received by the bootstrap node, query the HTTP server logs on the serving host. For example, if you are using an Apache web server to serve Ignition files, enter the following command:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span><span>grep</span> <span>-is</span> <span>'bootstrap.ign'</span> /var/log/httpd/access_log</code></pre> </div> </div> <div> <p>If the bootstrap Ignition file is received, the associated <code>HTTP GET</code> log message will include a <code>200 OK</code> success status, indicating that the request succeeded.</p> </div> </li> <li> <p>If the Ignition file was not received, check that the Ignition files exist and that they have the appropriate file and web server permissions on the serving host directly.</p> </li> </ol> </div> </li> <li> <p>If you are using a cloud provider mechanism to inject Ignition configuration files into hosts as part of their initial deployment.</p> <div> <ol type=\"a\"> <li> <p>Review the bootstrap node’s console to determine if the mechanism is injecting the bootstrap node Ignition file correctly.</p> </li> </ol> </div> </li> </ul> </div> </li> <li> <p>Verify the availability of the bootstrap node’s assigned storage device.</p> </li> <li> <p>Verify that the bootstrap node has been assigned an IP address from the DHCP server.</p> </li> <li> <p>Collect <code>bootkube.service</code> journald unit logs from the bootstrap node. Replace <code>&lt;bootstrap_fqdn&gt;</code> with the bootstrap node’s fully qualified domain name:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh core@&lt;bootstrap_fqdn&gt; journalctl <span>-b</span> <span>-f</span> <span>-u</span> bootkube.service</code></pre> </div> </div> <div> <table> <tbody><tr> <td> <i title=\"Note\"></i> </td> <td> <div> <p>The <code>bootkube.service</code> log on the bootstrap node outputs etcd <code>connection refused</code> errors, indicating that the bootstrap server is unable to connect to etcd on control plane nodes. After etcd has started on each control plane node and the nodes have joined the cluster, the errors should stop.</p> </div> </td> </tr> </tbody></table> </div> </li> <li> <p>Collect logs from the bootstrap node containers.</p> <div> <ol type=\"a\"> <li> <p>Collect the logs using <code>podman</code> on the bootstrap node. Replace <code>&lt;bootstrap_fqdn&gt;</code> with the bootstrap node’s fully qualified domain name:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh core@&lt;bootstrap_fqdn&gt; <span>'for pod in $(sudo podman ps -a -q); do sudo podman logs $pod; done'</span></code></pre> </div> </div> </li> </ol> </div> </li> <li> <p>If the bootstrap process fails, verify the following.</p> <div> <ul> <li> <p>You can resolve <code>api.&lt;cluster_name&gt;.&lt;base_domain&gt;</code> from the installation host.</p> </li> <li> <p>The load balancer proxies port 6443 connections to bootstrap and control plane nodes. Ensure that the proxy configuration meets OpenShift Container Platform installation requirements.</p> </li> </ul> </div> </li> </ol>",
            "number": 8,
            "type": "manual"
        },
        {
            "title": "Investigating control plane node installation issues",
            "description": "<p>If you experience control plane node installation issues, determine the control plane node OpenShift Container Platform software defined network (SDN), and network Operator status. Collect kubelet.service, crio.service journald unit logs, and control plane node container logs for visibility into control plane node agent, CRI-O container runtime, and pod activity.</p><ul> <li> <p>You have access to the cluster as a user with the <code>cluster-admin</code> role.</p> </li> <li> <p>You have installed the OpenShift CLI (<code>oc</code>).</p> </li> <li> <p>You have SSH access to your hosts.</p> </li> <li> <p>You have the fully qualified domain names of the bootstrap and control plane nodes.</p> </li> <li> <p>If you are hosting Ignition configuration files by using an HTTP server, you must have the HTTP server’s fully qualified domain name and the port number. You must also have SSH access to the HTTP host.</p> <div> <table> <tbody><tr> <td> <i title=\"Note\"></i> </td> <td> <div> <p>The initial <code>kubeadmin</code> password can be found in <code>&lt;install_directory&gt;/auth/kubeadmin-password</code> on the installation host.</p> </div> </td> </tr> </tbody></table> </div> </li> </ul><ol> <li> <p>If you have access to the console for the control plane node, monitor the console until the node reaches the login prompt. During the installation, Ignition log messages are output to the console.</p> </li> <li> <p>Verify Ignition file configuration.</p> <div> <ul> <li> <p>If you are hosting Ignition configuration files by using an HTTP server.</p> <div> <ol type=\"a\"> <li> <p>Verify the control plane node Ignition file URL. Replace <code>&lt;http_server_fqdn&gt;</code> with HTTP server’s fully qualified domain name:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>curl <span>-I</span> http://&lt;http_server_fqdn&gt;:&lt;port&gt;/master.ign  <i data-value=\"1\"></i><b>(1)</b></code></pre> </div> </div> <div> <table> <tbody><tr> <td><i data-value=\"1\"></i><b>1</b></td> <td>The <code>-I</code> option returns the header only. If the Ignition file is available on the specified URL, the command returns <code>200 OK</code> status. If it is not available, the command returns <code>404 file not found</code>.</td> </tr> </tbody></table> </div> </li> <li> <p>To verify that the Ignition file was received by the control plane node query the HTTP server logs on the serving host. For example, if you are using an Apache web server to serve Ignition files:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span><span>grep</span> <span>-is</span> <span>'master.ign'</span> /var/log/httpd/access_log</code></pre> </div> </div> <div> <p>If the master Ignition file is received, the associated <code>HTTP GET</code> log message will include a <code>200 OK</code> success status, indicating that the request succeeded.</p> </div> </li> <li> <p>If the Ignition file was not received, check that it exists on the serving host directly. Ensure that the appropriate file and web server permissions are in place.</p> </li> </ol> </div> </li> <li> <p>If you are using a cloud provider mechanism to inject Ignition configuration files into hosts as part of their initial deployment.</p> <div> <ol type=\"a\"> <li> <p>Review the console for the control plane node to determine if the mechanism is injecting the control plane node Ignition file correctly.</p> </li> </ol> </div> </li> </ul> </div> </li> <li> <p>Check the availability of the storage device assigned to the control plane node.</p> </li> <li> <p>Verify that the control plane node has been assigned an IP address from the DHCP server.</p> </li> <li> <p>Determine control plane node status.</p> <div> <ol type=\"a\"> <li> <p>Query control plane node status:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc get nodes</code></pre> </div> </div> </li> <li> <p>If one of the control plane nodes does not reach a <code>Ready</code> status, retrieve a detailed node description:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc describe node &lt;master_node&gt;</code></pre> </div> </div> <div> <table> <tbody><tr> <td> <i title=\"Note\"></i> </td> <td> <div> <p>It is not possible to run <code>oc</code> commands if an installation issue prevents the OpenShift Container Platform API from running or if the kubelet is not running yet on each node:</p> </div> </td> </tr> </tbody></table> </div> </li> </ol> </div> </li> <li> <p>Determine OpenShift Container Platform SDN status.</p> <div> <ol type=\"a\"> <li> <p>Review <code>sdn-controller</code>, <code>sdn</code>, and <code>ovs</code> daemon set status, in the <code>openshift-sdn</code> namespace:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc get daemonsets <span>-n</span> openshift-sdn</code></pre> </div> </div> </li> <li> <p>If those resources are listed as <code>Not found</code>, review pods in the <code>openshift-sdn</code> namespace:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc get pods <span>-n</span> openshift-sdn</code></pre> </div> </div> </li> <li> <p>Review logs relating to failed OpenShift Container Platform SDN pods in the <code>openshift-sdn</code> namespace:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc logs &lt;sdn_pod&gt; <span>-n</span> openshift-sdn</code></pre> </div> </div> </li> </ol> </div> </li> <li> <p>Determine cluster network configuration status.</p> <div> <ol type=\"a\"> <li> <p>Review whether the cluster’s network configuration exists:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc get network.config.openshift.io cluster <span>-o</span> yaml</code></pre> </div> </div> </li> <li> <p>If the installer failed to create the network configuration, generate the Kubernetes manifests again and review message output:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>./openshift-install create manifests</code></pre> </div> </div> </li> <li> <p>Review the pod status in the <code>openshift-network-operator</code> namespace to determine whether the Cluster Network Operator (CNO) is running:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc get pods <span>-n</span> openshift-network-operator</code></pre> </div> </div> </li> <li> <p>Gather network Operator pod logs from the <code>openshift-network-operator</code> namespace:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc logs pod/&lt;network_operator_pod_name&gt; <span>-n</span> openshift-network-operator</code></pre> </div> </div> </li> </ol> </div> </li> <li> <p>Monitor <code>kubelet.service</code> journald unit logs on control plane nodes, after they have booted. This provides visibility into control plane node agent activity.</p> <div> <ol type=\"a\"> <li> <p>Retrieve the logs using <code>oc</code>:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc adm node-logs <span>--role</span><span>=</span>master <span>-u</span> kubelet</code></pre> </div> </div> </li> <li> <p>If the API is not functional, review the logs using SSH instead. Replace <code>&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code> with appropriate values:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; journalctl <span>-b</span> <span>-f</span> <span>-u</span> kubelet.service</code></pre> </div> </div> <div> <table> <tbody><tr> <td> <i title=\"Note\"></i> </td> <td> <div> <p>OpenShift Container Platform 4.9 cluster nodes running Red Hat Enterprise Linux CoreOS (RHCOS) are immutable and rely on Operators to apply cluster changes. Accessing cluster nodes using SSH is not recommended and nodes will be tainted as <em>accessed</em>. Before attempting to collect diagnostic data over SSH, review whether the data collected by running <code>oc adm must gather</code> and other <code>oc</code> commands is sufficient instead. However, if the OpenShift Container Platform API is not available, or the kubelet is not properly functioning on the target node, <code>oc</code> operations will be impacted. In such situations, it is possible to access nodes using <code>ssh core@&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code>.</p> </div> </td> </tr> </tbody></table> </div> </li> </ol> </div> </li> <li> <p>Retrieve <code>crio.service</code> journald unit logs on control plane nodes, after they have booted. This provides visibility into control plane node CRI-O container runtime activity.</p> <div> <ol type=\"a\"> <li> <p>Retrieve the logs using <code>oc</code>:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc adm node-logs <span>--role</span><span>=</span>master <span>-u</span> crio</code></pre> </div> </div> </li> <li> <p>If the API is not functional, review the logs using SSH instead:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; journalctl <span>-b</span> <span>-f</span> <span>-u</span> crio.service</code></pre> </div> </div> </li> </ol> </div> </li> <li> <p>Collect logs from specific subdirectories under <code>/var/log/</code> on control plane nodes.</p> <div> <ol type=\"a\"> <li> <p>Retrieve a list of logs contained within a <code>/var/log/</code> subdirectory. The following example lists files in <code>/var/log/openshift-apiserver/</code> on all control plane nodes:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc adm node-logs <span>--role</span><span>=</span>master <span>--path</span><span>=</span>openshift-apiserver</code></pre> </div> </div> </li> <li> <p>Inspect a specific log within a <code>/var/log/</code> subdirectory. The following example outputs <code>/var/log/openshift-apiserver/audit.log</code> contents from all control plane nodes:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc adm node-logs <span>--role</span><span>=</span>master <span>--path</span><span>=</span>openshift-apiserver/audit.log</code></pre> </div> </div> </li> <li> <p>If the API is not functional, review the logs on each node using SSH instead. The following example tails <code>/var/log/openshift-apiserver/audit.log</code>:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; <span>sudo tail</span> <span>-f</span> /var/log/openshift-apiserver/audit.log</code></pre> </div> </div> </li> </ol> </div> </li> <li> <p>Review control plane node container logs using SSH.</p> <div> <ol type=\"a\"> <li> <p>List the containers:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; <span>sudo </span>crictl ps <span>-a</span></code></pre> </div> </div> </li> <li> <p>Retrieve a container’s logs using <code>crictl</code>:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; <span>sudo </span>crictl logs <span>-f</span> &lt;container_id&gt;</code></pre> </div> </div> </li> </ol> </div> </li> <li> <p>If you experience control plane node configuration issues, verify that the MCO, MCO endpoint, and DNS record are functioning. The Machine Config Operator (MCO) manages operating system configuration during the installation procedure. Also verify system clock accuracy and certificate validity.</p> <div> <ol type=\"a\"> <li> <p>Test whether the MCO endpoint is available. Replace <code>&lt;cluster_name&gt;</code> with appropriate values:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>curl https://api-int.&lt;cluster_name&gt;:22623/config/master</code></pre> </div> </div> </li> <li> <p>If the endpoint is unresponsive, verify load balancer configuration. Ensure that the endpoint is configured to run on port 22623.</p> </li> <li> <p>Verify that the MCO endpoint’s DNS record is configured and resolves to the load balancer.</p> <div> <ol type=\"i\"> <li> <p>Run a DNS lookup for the defined MCO endpoint name:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>dig api-int.&lt;cluster_name&gt; @&lt;dns_server&gt;</code></pre> </div> </div> </li> <li> <p>Run a reverse lookup to the assigned MCO IP address on the load balancer:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>dig <span>-x</span> &lt;load_balancer_mco_ip_address&gt; @&lt;dns_server&gt;</code></pre> </div> </div> </li> </ol> </div> </li> <li> <p>Verify that the MCO is functioning from the bootstrap node directly. Replace <code>&lt;bootstrap_fqdn&gt;</code> with the bootstrap node’s fully qualified domain name:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh core@&lt;bootstrap_fqdn&gt; curl https://api-int.&lt;cluster_name&gt;:22623/config/master</code></pre> </div> </div> </li> <li> <p>System clock time must be synchronized between bootstrap, master, and worker nodes. Check each node’s system clock reference time and time synchronization statistics:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh core@&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; chronyc tracking</code></pre> </div> </div> </li> <li> <p>Review certificate validity:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>openssl s_client <span>-connect</span> api-int.&lt;cluster_name&gt;:22623 | openssl x509 <span>-noout</span> <span>-text</span></code></pre> </div> </div> </li> </ol> </div> </li> </ol>",
            "number": 9,
            "type": "manual"
        },
        {
            "title": "Investigating etcd installation issues",
            "description": "<p>If you experience etcd issues during installation, you can check etcd pod status and collect etcd pod logs. You can also verify etcd DNS records and check DNS availability on control plane nodes.</p><ul> <li> <p>You have access to the cluster as a user with the <code>cluster-admin</code> role.</p> </li> <li> <p>You have installed the OpenShift CLI (<code>oc</code>).</p> </li> <li> <p>You have SSH access to your hosts.</p> </li> <li> <p>You have the fully qualified domain names of the control plane nodes.</p> </li> </ul><ol> <li> <p>Check the status of etcd pods.</p> <div> <ol type=\"a\"> <li> <p>Review the status of pods in the <code>openshift-etcd</code> namespace:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc get pods <span>-n</span> openshift-etcd</code></pre> </div> </div> </li> <li> <p>Review the status of pods in the <code>openshift-etcd-operator</code> namespace:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc get pods <span>-n</span> openshift-etcd-operator</code></pre> </div> </div> </li> </ol> </div> </li> <li> <p>If any of the pods listed by the previous commands are not showing a <code>Running</code> or a <code>Completed</code> status, gather diagnostic information for the pod.</p> <div> <ol type=\"a\"> <li> <p>Review events for the pod:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc describe pod/&lt;pod_name&gt; <span>-n</span> &lt;namespace&gt;</code></pre> </div> </div> </li> <li> <p>Inspect the pod’s logs:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc logs pod/&lt;pod_name&gt; <span>-n</span> &lt;namespace&gt;</code></pre> </div> </div> </li> <li> <p>If the pod has more than one container, the preceding command will create an error, and the container names will be provided in the error message. Inspect logs for each container:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc logs pod/&lt;pod_name&gt; <span>-c</span> &lt;container_name&gt; <span>-n</span> &lt;namespace&gt;</code></pre> </div> </div> </li> </ol> </div> </li> <li> <p>If the API is not functional, review etcd pod and container logs on each control plane node by using SSH instead. Replace <code>&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code> with appropriate values.</p> <div> <ol type=\"a\"> <li> <p>List etcd pods on each control plane node:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; <span>sudo </span>crictl pods <span>--name</span><span>=</span>etcd-</code></pre> </div> </div> </li> <li> <p>For any pods not showing <code>Ready</code> status, inspect pod status in detail. Replace <code>&lt;pod_id&gt;</code> with the pod’s ID listed in the output of the preceding command:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; <span>sudo </span>crictl inspectp &lt;pod_id&gt;</code></pre> </div> </div> </li> <li> <p>List containers related to a pod:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; <span>sudo </span>crictl ps | <span>grep</span> <span>'&lt;pod_id&gt;'</span></code></pre> </div> </div> </li> <li> <p>For any containers not showing <code>Ready</code> status, inspect container status in detail. Replace <code>&lt;container_id&gt;</code> with container IDs listed in the output of the preceding command:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; <span>sudo </span>crictl inspect &lt;container_id&gt;</code></pre> </div> </div> </li> <li> <p>Review the logs for any containers not showing a <code>Ready</code> status. Replace <code>&lt;container_id&gt;</code> with the container IDs listed in the output of the preceding command:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; <span>sudo </span>crictl logs <span>-f</span> &lt;container_id&gt;</code></pre> </div> </div> <div> <table> <tbody><tr> <td> <i title=\"Note\"></i> </td> <td> <div> <p>OpenShift Container Platform 4.9 cluster nodes running Red Hat Enterprise Linux CoreOS (RHCOS) are immutable and rely on Operators to apply cluster changes. Accessing cluster nodes using SSH is not recommended and nodes will be tainted as <em>accessed</em>. Before attempting to collect diagnostic data over SSH, review whether the data collected by running <code>oc adm must gather</code> and other <code>oc</code> commands is sufficient instead. However, if the OpenShift Container Platform API is not available, or the kubelet is not properly functioning on the target node, <code>oc</code> operations will be impacted. In such situations, it is possible to access nodes using <code>ssh core@&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code>.</p> </div> </td> </tr> </tbody></table> </div> </li> </ol> </div> </li> <li> <p>Validate primary and secondary DNS server connectivity from control plane nodes.</p> </li> </ol>",
            "number": 10,
            "type": "manual"
        },
        {
            "title": "Investigating control plane node kubelet and API server issues",
            "description": "<p>To investigate control plane node kubelet and API server issues during installation, check DNS, DHCP, and load balancer functionality. Also, verify that certificates have not expired.</p><ul> <li> <p>You have access to the cluster as a user with the <code>cluster-admin</code> role.</p> </li> <li> <p>You have installed the OpenShift CLI (<code>oc</code>).</p> </li> <li> <p>You have SSH access to your hosts.</p> </li> <li> <p>You have the fully qualified domain names of the control plane nodes.</p> </li> </ul><ol> <li> <p>Verify that the API server’s DNS record directs the kubelet on control plane nodes to <code>https://api-int.&lt;cluster_name&gt;.&lt;base_domain&gt;:6443</code>. Ensure that the record references the load balancer.</p> </li> <li> <p>Ensure that the load balancer’s port 6443 definition references each control plane node.</p> </li> <li> <p>Check that unique control plane node hostnames have been provided by DHCP.</p> </li> <li> <p>Inspect the <code>kubelet.service</code> journald unit logs on each control plane node.</p> <div> <ol type=\"a\"> <li> <p>Retrieve the logs using <code>oc</code>:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc adm node-logs <span>--role</span><span>=</span>master <span>-u</span> kubelet</code></pre> </div> </div> </li> <li> <p>If the API is not functional, review the logs using SSH instead. Replace <code>&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code> with appropriate values:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; journalctl <span>-b</span> <span>-f</span> <span>-u</span> kubelet.service</code></pre> </div> </div> <div> <table> <tbody><tr> <td> <i title=\"Note\"></i> </td> <td> <div> <p>OpenShift Container Platform 4.9 cluster nodes running Red Hat Enterprise Linux CoreOS (RHCOS) are immutable and rely on Operators to apply cluster changes. Accessing cluster nodes using SSH is not recommended and nodes will be tainted as <em>accessed</em>. Before attempting to collect diagnostic data over SSH, review whether the data collected by running <code>oc adm must gather</code> and other <code>oc</code> commands is sufficient instead. However, if the OpenShift Container Platform API is not available, or the kubelet is not properly functioning on the target node, <code>oc</code> operations will be impacted. In such situations, it is possible to access nodes using <code>ssh core@&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code>.</p> </div> </td> </tr> </tbody></table> </div> </li> </ol> </div> </li> <li> <p>Check for certificate expiration messages in the control plane node kubelet logs.</p> <div> <ol type=\"a\"> <li> <p>Retrieve the log using <code>oc</code>:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc adm node-logs <span>--role</span><span>=</span>master <span>-u</span> kubelet | <span>grep</span> <span>-is</span> <span>'x509: certificate has expired'</span></code></pre> </div> </div> </li> <li> <p>If the API is not functional, review the logs using SSH instead. Replace <code>&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code> with appropriate values:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; journalctl <span>-b</span> <span>-f</span> <span>-u</span> kubelet.service  | <span>grep</span> <span>-is</span> <span>'x509: certificate has expired'</span></code></pre> </div> </div> </li> </ol> </div> </li> </ol>",
            "number": 11,
            "type": "manual"
        },
        {
            "title": "Investigating worker node installation issues",
            "description": "<p>If you experience worker node installation issues, you can review the worker node status. Collect kubelet.service, crio.service journald unit logs and the worker node container logs for visibility into the worker node agent, CRI-O container runtime and pod activity. Additionally, you can check the Ignition file and Machine API Operator functionality. If worker node post-installation configuration fails, check Machine Config Operator (MCO) and DNS functionality. You can also verify system clock synchronization between the bootstrap, master, and worker nodes, and validate certificates.</p><ul> <li> <p>You have access to the cluster as a user with the <code>cluster-admin</code> role.</p> </li> <li> <p>You have installed the OpenShift CLI (<code>oc</code>).</p> </li> <li> <p>You have SSH access to your hosts.</p> </li> <li> <p>You have the fully qualified domain names of the bootstrap and worker nodes.</p> </li> <li> <p>If you are hosting Ignition configuration files by using an HTTP server, you must have the HTTP server’s fully qualified domain name and the port number. You must also have SSH access to the HTTP host.</p> <div> <table> <tbody><tr> <td> <i title=\"Note\"></i> </td> <td> <div> <p>The initial <code>kubeadmin</code> password can be found in <code>&lt;install_directory&gt;/auth/kubeadmin-password</code> on the installation host.</p> </div> </td> </tr> </tbody></table> </div> </li> </ul><ol> <li> <p>If you have access to the worker node’s console, monitor the console until the node reaches the login prompt. During the installation, Ignition log messages are output to the console.</p> </li> <li> <p>Verify Ignition file configuration.</p> <div> <ul> <li> <p>If you are hosting Ignition configuration files by using an HTTP server.</p> <div> <ol type=\"a\"> <li> <p>Verify the worker node Ignition file URL. Replace <code>&lt;http_server_fqdn&gt;</code> with HTTP server’s fully qualified domain name:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>curl <span>-I</span> http://&lt;http_server_fqdn&gt;:&lt;port&gt;/worker.ign  <i data-value=\"1\"></i><b>(1)</b></code></pre> </div> </div> <div> <table> <tbody><tr> <td><i data-value=\"1\"></i><b>1</b></td> <td>The <code>-I</code> option returns the header only. If the Ignition file is available on the specified URL, the command returns <code>200 OK</code> status. If it is not available, the command returns <code>404 file not found</code>.</td> </tr> </tbody></table> </div> </li> <li> <p>To verify that the Ignition file was received by the worker node, query the HTTP server logs on the HTTP host. For example, if you are using an Apache web server to serve Ignition files:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span><span>grep</span> <span>-is</span> <span>'worker.ign'</span> /var/log/httpd/access_log</code></pre> </div> </div> <div> <p>If the worker Ignition file is received, the associated <code>HTTP GET</code> log message will include a <code>200 OK</code> success status, indicating that the request succeeded.</p> </div> </li> <li> <p>If the Ignition file was not received, check that it exists on the serving host directly. Ensure that the appropriate file and web server permissions are in place.</p> </li> </ol> </div> </li> <li> <p>If you are using a cloud provider mechanism to inject Ignition configuration files into hosts as part of their initial deployment.</p> <div> <ol type=\"a\"> <li> <p>Review the worker node’s console to determine if the mechanism is injecting the worker node Ignition file correctly.</p> </li> </ol> </div> </li> </ul> </div> </li> <li> <p>Check the availability of the worker node’s assigned storage device.</p> </li> <li> <p>Verify that the worker node has been assigned an IP address from the DHCP server.</p> </li> <li> <p>Determine worker node status.</p> <div> <ol type=\"a\"> <li> <p>Query node status:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc get nodes</code></pre> </div> </div> </li> <li> <p>Retrieve a detailed node description for any worker nodes not showing a <code>Ready</code> status:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc describe node &lt;worker_node&gt;</code></pre> </div> </div> <div> <table> <tbody><tr> <td> <i title=\"Note\"></i> </td> <td> <div> <p>It is not possible to run <code>oc</code> commands if an installation issue prevents the OpenShift Container Platform API from running or if the kubelet is not running yet on each node.</p> </div> </td> </tr> </tbody></table> </div> </li> </ol> </div> </li> <li> <p>Unlike control plane nodes, worker nodes are deployed and scaled using the Machine API Operator. Check the status of the Machine API Operator.</p> <div> <ol type=\"a\"> <li> <p>Review Machine API Operator pod status:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc get pods <span>-n</span> openshift-machine-api</code></pre> </div> </div> </li> <li> <p>If the Machine API Operator pod does not have a <code>Ready</code> status, detail the pod’s events:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc describe pod/&lt;machine_api_operator_pod_name&gt; <span>-n</span> openshift-machine-api</code></pre> </div> </div> </li> <li> <p>Inspect <code>machine-api-operator</code> container logs. The container runs within the <code>machine-api-operator</code> pod:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc logs pod/&lt;machine_api_operator_pod_name&gt; <span>-n</span> openshift-machine-api <span>-c</span> machine-api-operator</code></pre> </div> </div> </li> <li> <p>Also inspect <code>kube-rbac-proxy</code> container logs. The container also runs within the <code>machine-api-operator</code> pod:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc logs pod/&lt;machine_api_operator_pod_name&gt; <span>-n</span> openshift-machine-api <span>-c</span> kube-rbac-proxy</code></pre> </div> </div> </li> </ol> </div> </li> <li> <p>Monitor <code>kubelet.service</code> journald unit logs on worker nodes, after they have booted. This provides visibility into worker node agent activity.</p> <div> <ol type=\"a\"> <li> <p>Retrieve the logs using <code>oc</code>:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc adm node-logs <span>--role</span><span>=</span>worker <span>-u</span> kubelet</code></pre> </div> </div> </li> <li> <p>If the API is not functional, review the logs using SSH instead. Replace <code>&lt;worker-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code> with appropriate values:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh core@&lt;worker-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; journalctl <span>-b</span> <span>-f</span> <span>-u</span> kubelet.service</code></pre> </div> </div> <div> <table> <tbody><tr> <td> <i title=\"Note\"></i> </td> <td> <div> <p>OpenShift Container Platform 4.9 cluster nodes running Red Hat Enterprise Linux CoreOS (RHCOS) are immutable and rely on Operators to apply cluster changes. Accessing cluster nodes using SSH is not recommended and nodes will be tainted as <em>accessed</em>. Before attempting to collect diagnostic data over SSH, review whether the data collected by running <code>oc adm must gather</code> and other <code>oc</code> commands is sufficient instead. However, if the OpenShift Container Platform API is not available, or the kubelet is not properly functioning on the target node, <code>oc</code> operations will be impacted. In such situations, it is possible to access nodes using <code>ssh core@&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code>.</p> </div> </td> </tr> </tbody></table> </div> </li> </ol> </div> </li> <li> <p>Retrieve <code>crio.service</code> journald unit logs on worker nodes, after they have booted. This provides visibility into worker node CRI-O container runtime activity.</p> <div> <ol type=\"a\"> <li> <p>Retrieve the logs using <code>oc</code>:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc adm node-logs <span>--role</span><span>=</span>worker <span>-u</span> crio</code></pre> </div> </div> </li> <li> <p>If the API is not functional, review the logs using SSH instead:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh core@&lt;worker-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; journalctl <span>-b</span> <span>-f</span> <span>-u</span> crio.service</code></pre> </div> </div> </li> </ol> </div> </li> <li> <p>Collect logs from specific subdirectories under <code>/var/log/</code> on worker nodes.</p> <div> <ol type=\"a\"> <li> <p>Retrieve a list of logs contained within a <code>/var/log/</code> subdirectory. The following example lists files in <code>/var/log/sssd/</code> on all worker nodes:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc adm node-logs <span>--role</span><span>=</span>worker <span>--path</span><span>=</span>sssd</code></pre> </div> </div> </li> <li> <p>Inspect a specific log within a <code>/var/log/</code> subdirectory. The following example outputs <code>/var/log/sssd/audit.log</code> contents from all worker nodes:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc adm node-logs <span>--role</span><span>=</span>worker <span>--path</span><span>=</span>sssd/sssd.log</code></pre> </div> </div> </li> <li> <p>If the API is not functional, review the logs on each node using SSH instead. The following example tails <code>/var/log/sssd/sssd.log</code>:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh core@&lt;worker-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; <span>sudo tail</span> <span>-f</span> /var/log/sssd/sssd.log</code></pre> </div> </div> </li> </ol> </div> </li> <li> <p>Review worker node container logs using SSH.</p> <div> <ol type=\"a\"> <li> <p>List the containers:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh core@&lt;worker-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; <span>sudo </span>crictl ps <span>-a</span></code></pre> </div> </div> </li> <li> <p>Retrieve a container’s logs using <code>crictl</code>:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh core@&lt;worker-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; <span>sudo </span>crictl logs <span>-f</span> &lt;container_id&gt;</code></pre> </div> </div> </li> </ol> </div> </li> <li> <p>If you experience worker node configuration issues, verify that the MCO, MCO endpoint, and DNS record are functioning. The Machine Config Operator (MCO) manages operating system configuration during the installation procedure. Also verify system clock accuracy and certificate validity.</p> <div> <ol type=\"a\"> <li> <p>Test whether the MCO endpoint is available. Replace <code>&lt;cluster_name&gt;</code> with appropriate values:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>curl https://api-int.&lt;cluster_name&gt;:22623/config/worker</code></pre> </div> </div> </li> <li> <p>If the endpoint is unresponsive, verify load balancer configuration. Ensure that the endpoint is configured to run on port 22623.</p> </li> <li> <p>Verify that the MCO endpoint’s DNS record is configured and resolves to the load balancer.</p> <div> <ol type=\"i\"> <li> <p>Run a DNS lookup for the defined MCO endpoint name:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>dig api-int.&lt;cluster_name&gt; @&lt;dns_server&gt;</code></pre> </div> </div> </li> <li> <p>Run a reverse lookup to the assigned MCO IP address on the load balancer:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>dig <span>-x</span> &lt;load_balancer_mco_ip_address&gt; @&lt;dns_server&gt;</code></pre> </div> </div> </li> </ol> </div> </li> <li> <p>Verify that the MCO is functioning from the bootstrap node directly. Replace <code>&lt;bootstrap_fqdn&gt;</code> with the bootstrap node’s fully qualified domain name:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh core@&lt;bootstrap_fqdn&gt; curl https://api-int.&lt;cluster_name&gt;:22623/config/worker</code></pre> </div> </div> </li> <li> <p>System clock time must be synchronized between bootstrap, master, and worker nodes. Check each node’s system clock reference time and time synchronization statistics:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>ssh core@&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; chronyc tracking</code></pre> </div> </div> </li> <li> <p>Review certificate validity:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>openssl s_client <span>-connect</span> api-int.&lt;cluster_name&gt;:22623 | openssl x509 <span>-noout</span> <span>-text</span></code></pre> </div> </div> </li> </ol> </div> </li> </ol>",
            "number": 12,
            "type": "manual"
        },
        {
            "title": "Querying Operator status after installation",
            "description": "<p>You can check Operator status at the end of an installation. Retrieve diagnostic data for Operators that do not become available. Review logs for any Operator pods that are listed as Pending or have an error status. Validate base images used by problematic pods.</p><ul> <li> <p>You have access to the cluster as a user with the <code>cluster-admin</code> role.</p> </li> <li> <p>You have installed the OpenShift CLI (<code>oc</code>).</p> </li> </ul><ol> <li> <p>Check that cluster Operators are all available at the end of an installation.</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc get clusteroperators</code></pre> </div> </div> </li> <li> <p>Verify that all of the required certificate signing requests (CSRs) are approved. Some nodes might not move to a <code>Ready</code> status and some cluster Operators might not become available if there are pending CSRs.</p> <div> <ol type=\"a\"> <li> <p>Check the status of the CSRs and ensure that you see a client and server request with the <code>Pending</code> or <code>Approved</code> status for each machine that you added to the cluster:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc get csr</code></pre> </div> </div> <div> <div>Example output</div> <div> <pre><code data-lang=\"terminal\"><span>NAME        AGE     REQUESTOR                                                                   CONDITION\ncsr-8b2br   15m     system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending <i data-value=\"1\"></i><b>(1)</b>\ncsr-8vnps   15m     system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending\ncsr-bfd72   5m26s   system:node:ip-10-0-50-126.us-east-2.compute.internal                       Pending <i data-value=\"2\"></i><b>(2)</b>\ncsr-c57lv   5m26s   system:node:ip-10-0-95-157.us-east-2.compute.internal                       Pending\n</span><span>...</span></code></pre> </div> </div> <div> <table> <tbody><tr> <td><i data-value=\"1\"></i><b>1</b></td> <td>A client request CSR.</td> </tr> <tr> <td><i data-value=\"2\"></i><b>2</b></td> <td>A server request CSR.</td> </tr> </tbody></table> </div> <div> <p>In this example, two machines are joining the cluster. You might see more approved CSRs in the list.</p> </div> </li> <li> <p>If the CSRs were not approved, after all of the pending CSRs for the machines you added are in <code>Pending</code> status, approve the CSRs for your cluster machines:</p> <div> <table> <tbody><tr> <td> <i title=\"Note\"></i> </td> <td> <div> <p>Because the CSRs rotate automatically, approve your CSRs within an hour of adding the machines to the cluster. If you do not approve them within an hour, the certificates will rotate, and more than two certificates will be present for each node. You must approve all of these certificates. After you approve the initial CSRs, the subsequent node client CSRs are automatically approved by the cluster <code>kube-controller-manager</code>.</p> </div> </td> </tr> </tbody></table> </div> <div> <table> <tbody><tr> <td> <i title=\"Note\"></i> </td> <td> <div> <p>For clusters running on platforms that are not machine API enabled, such as bare metal and other user-provisioned infrastructure, you must implement a method of automatically approving the kubelet serving certificate requests (CSRs). If a request is not approved, then the <code>oc exec</code>, <code>oc rsh</code>, and <code>oc logs</code> commands cannot succeed, because a serving certificate is required when the API server connects to the kubelet. Any operation that contacts the Kubelet endpoint requires this certificate approval to be in place. The method must watch for new CSRs, confirm that the CSR was submitted by the <code>node-bootstrapper</code> service account in the <code>system:node</code> or <code>system:admin</code> groups, and confirm the identity of the node.</p> </div> </td> </tr> </tbody></table> </div> <div> <ul> <li> <p>To approve them individually, run the following command for each valid CSR:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc adm certificate approve &lt;csr_name&gt; <i data-value=\"1\"></i><b>(1)</b></code></pre> </div> </div> <div> <table> <tbody><tr> <td><i data-value=\"1\"></i><b>1</b></td> <td><code>&lt;csr_name&gt;</code> is the name of a CSR from the list of current CSRs.</td> </tr> </tbody></table> </div> </li> <li> <p>To approve all pending CSRs, run the following command:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc get csr <span>-o</span> go-template<span>=</span><span>'{{range .items}}{{if not .status}}{{.metadata.name}}{{\"\\n\"}}{{end}}{{end}}'</span> | xargs oc adm certificate approve</code></pre> </div> </div> </li> </ul> </div> </li> </ol> </div> </li> <li> <p>View Operator events:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc describe clusteroperator &lt;operator_name&gt;</code></pre> </div> </div> </li> <li> <p>Review Operator pod status within the Operator’s namespace:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc get pods <span>-n</span> &lt;operator_namespace&gt;</code></pre> </div> </div> </li> <li> <p>Obtain a detailed description for pods that do not have <code>Running</code> status:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc describe pod/&lt;operator_pod_name&gt; <span>-n</span> &lt;operator_namespace&gt;</code></pre> </div> </div> </li> <li> <p>Inspect pod logs:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc logs pod/&lt;operator_pod_name&gt; <span>-n</span> &lt;operator_namespace&gt;</code></pre> </div> </div> </li> <li> <p>When experiencing pod base image related issues, review base image status.</p> <div> <ol type=\"a\"> <li> <p>Obtain details of the base image used by a problematic pod:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc get pod <span>-o</span> <span>\"jsonpath={range .status.containerStatuses[*]}{.name}{'</span><span>\\t</span><span>'}{.state}{'</span><span>\\t</span><span>'}{.image}{'</span><span>\\n</span><span>'}{end}\"</span> &lt;operator_pod_name&gt; <span>-n</span> &lt;operator_namespace&gt;</code></pre> </div> </div> </li> <li> <p>List base image release information:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>oc adm release info &lt;image_path&gt;:&lt;tag&gt; <span>--commits</span></code></pre> </div> </div> </li> </ol> </div> </li> </ol>",
            "number": 13,
            "type": "manual"
        },
        {
            "title": "Gathering logs from a failed installation",
            "description": "<p>If you gave an SSH key to your installation program, you can gather data about your failed installation.</p><figure class=\"table\"><table><tbody><tr><td></td><td>You use a different command to gather logs about an unsuccessful installation than to gather logs from a running cluster. If you must gather logs from a running cluster, use the oc adm must-gather command.</td></tr></tbody></table></figure><ul> <li> <p>Your OpenShift Container Platform installation failed before the bootstrap process finished. The bootstrap node is running and accessible through SSH.</p> </li> <li> <p>The <code>ssh-agent</code> process is active on your computer, and you provided the same SSH key to both the <code>ssh-agent</code> process and the installation program.</p> </li> <li> <p>If you tried to install a cluster on infrastructure that you provisioned, you must have the fully qualified domain names of the bootstrap and control plane nodes.</p> </li> </ul><ol> <li> <p>Generate the commands that are required to obtain the installation logs from the bootstrap and control plane machines:</p> <div> <div> <div> <ul> <li> <p>If you used installer-provisioned infrastructure, change to the directory that contains the installation program and run the following command:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>./openshift-install gather bootstrap <span>--dir</span> &lt;installation_directory&gt; <i data-value=\"1\"></i><b>(1)</b></code></pre> </div> </div> <div> <table> <tbody><tr> <td><i data-value=\"1\"></i><b>1</b></td> <td><code>installation_directory</code> is the directory you specified when you ran <code>./openshift-install create cluster</code>. This directory contains the OpenShift Container Platform definition files that the installation program creates.</td> </tr> </tbody></table> </div> <div> <p>For installer-provisioned infrastructure, the installation program stores information about the cluster, so you do not specify the hostnames or IP addresses.</p> </div> </li> <li> <p>If you used infrastructure that you provisioned yourself, change to the directory that contains the installation program and run the following command:</p> <div> <div> <pre><code data-lang=\"terminal\"><span>$</span><span> </span>./openshift-install gather bootstrap <span>--dir</span> &lt;installation_directory&gt; <span>\\ </span><i data-value=\"1\"></i><b>(1)</b>\n<span>    --bootstrap &lt;bootstrap_address&gt;</span><span> </span><span>\\ </span><i data-value=\"2\"></i><b>(2)</b>\n<span>    --master &lt;master_1_address&gt;</span><span> </span><span>\\ </span><i data-value=\"3\"></i><b>(3)</b>\n<span>    --master &lt;master_2_address&gt;</span><span> </span><span>\\ </span><i data-value=\"3\"></i><b>(3)</b>\n<span>    --master &lt;master_3_address&gt;</span><span>\" <i data-value=\"3\"></i><b>(3)</b>\n</span></code></pre> </div> </div> <div> <table> <tbody><tr> <td><i data-value=\"1\"></i><b>1</b></td> <td>For <code>installation_directory</code>, specify the same directory you specified when you ran <code>./openshift-install create cluster</code>. This directory contains the OpenShift Container Platform definition files that the installation program creates.</td> </tr> <tr> <td><i data-value=\"2\"></i><b>2</b></td> <td><code>&lt;bootstrap_address&gt;</code> is the fully qualified domain name or IP address of the cluster’s bootstrap machine.</td> </tr> <tr> <td><i data-value=\"3\"></i><b>3</b></td> <td>For each control plane, or master, machine in your cluster, replace <code>&lt;master_*_address&gt;</code> with its fully qualified domain name or IP address.</td> </tr> </tbody></table> </div> <div> <table> <tbody><tr> <td> <i title=\"Note\"></i> </td> <td> <div> <p>A default cluster contains three control plane machines. List all of your control plane machines as shown, no matter how many your cluster uses.</p> </div> </td> </tr> </tbody></table> </div> </li> </ul> </div> </div> </div> <div> <div>Example output</div> <div> <pre><code data-lang=\"terminal\"><span>INFO Pulling debug logs from the bootstrap machine\n</span><span>INFO Bootstrap gather logs captured here \"&lt;installation_directory&gt;</span>/log-bundle-&lt;timestamp&gt;.tar.gz<span>\"</span></code></pre> </div> </div> <div> <p>If you open a Red Hat support case about your installation failure, include the compressed logs in the case.</p> </div> </li> </ol>",
            "number": 14,
            "type": "manual"
        },
        {
            "title": "Additional resources",
            "description": "<ul> <li> <p>See <a href=\"https://docs.openshift.com/container-platform/4.9/support/troubleshooting/../../architecture/architecture-installation.html#installation-process_architecture-installation\">Installation process</a> for more details on OpenShift Container Platform installation types and process.</p> </li> </ul>",
            "number": 15,
            "type": "manual"
        }
    ],
    "tags": [
        "converted-ocp-troubleshooting"
    ]
}
